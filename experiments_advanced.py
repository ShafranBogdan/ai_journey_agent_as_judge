#!/usr/bin/env python3
"""
Advanced Experiments for Agent-as-Judge
=======================================

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏:
- Data augmentation
- Multi-task learning 
- Knowledge distillation
- Advanced regularization
"""

import os
import pandas as pd
import numpy as np
import torch
from typing import Dict, List, Tuple, Optional
import random
import re
from datasets import Dataset
from transformers import AutoTokenizer
from unsloth import FastModel
import warnings
warnings.filterwarnings("ignore")


class DataAugmenter:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, seed: int = 42):
        self.seed = seed
        random.seed(seed)
        np.random.seed(seed)
    
    def paraphrase_prompts(self, df: pd.DataFrame, augment_ratio: float = 0.3) -> pd.DataFrame:
        """–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
        
        paraphrase_templates = {
            "–∑–∞–¥–∞–Ω–∏–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏": [
                "–ó–∞–¥–∞—á–∞ –¥–ª—è –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è",
                "–ü—Ä–∏–º–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", 
                "–°–ª—É—á–∞–π –¥–ª—è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è",
                "–ú–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è –æ—Ü–µ–Ω–∫–∏"
            ],
            "—ç—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç": [
                "–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç",
                "–í–µ—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç",
                "–û–±—Ä–∞–∑—Ü–æ–≤—ã–π –æ—Ç–≤–µ—Ç",
                "–ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç"
            ],
            "–æ—Ç–≤–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏": [
                "–ü—Ä–æ–≤–µ—Ä—è–µ–º—ã–π –æ—Ç–≤–µ—Ç",
                "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–π –æ—Ç–≤–µ—Ç",
                "–û—Ü–µ–Ω–∏–≤–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç",
                "–ò—Å—Å–ª–µ–¥—É–µ–º—ã–π –æ—Ç–≤–µ—Ç"
            ],
            "–∫—Ä–∏—Ç–µ—Ä–∏–π –æ—Ü–µ–Ω–∫–∏": [
                "–ö—Ä–∏—Ç–µ—Ä–∏–π –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è",
                "–ü–∞—Ä–∞–º–µ—Ç—Ä –æ—Ü–µ–Ω–∫–∏",
                "–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞",
                "–ú–µ—Ä–∏–ª–æ –æ—Ü–µ–Ω–∫–∏"
            ]
        }
        
        augmented_data = []
        num_to_augment = int(len(df) * augment_ratio)
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        indices_to_augment = random.sample(range(len(df)), num_to_augment)
        
        for idx in indices_to_augment:
            row = df.iloc[idx].copy()
            prompt = row['prompt']
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
            for original, variants in paraphrase_templates.items():
                if original in prompt.lower():
                    replacement = random.choice(variants)
                    # –ó–∞–º–µ–Ω—è–µ–º —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞
                    prompt = re.sub(
                        re.escape(original), 
                        replacement, 
                        prompt, 
                        flags=re.IGNORECASE
                    )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ –≤ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
            variations = [
                ("–í—ã–ø–æ–ª–Ω–∏", "–í—ã–ø–æ–ª–Ω–∏—Ç–µ"),
                ("–ù–∞–π–¥–∏", "–ù–∞–π–¥–∏—Ç–µ"), 
                ("–†–µ—à–∏", "–†–µ—à–∏—Ç–µ"),
                ("–û–ø—Ä–µ–¥–µ–ª–∏", "–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ")
            ]
            
            for old, new in variations:
                if random.random() < 0.3:  # 30% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∑–∞–º–µ–Ω—ã
                    prompt = prompt.replace(old, new)
            
            row['prompt'] = prompt
            augmented_data.append(row)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        augmented_df = pd.concat([df, pd.DataFrame(augmented_data)], ignore_index=True)
        return augmented_df.sample(frac=1, random_state=self.seed)  # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
    
    def add_noise_to_scores(self, df: pd.DataFrame, noise_ratio: float = 0.1) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —à—É–º –∫ –æ—Ü–µ–Ω–∫–∞–º –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏"""
        
        df_noisy = df.copy()
        num_to_noise = int(len(df) * noise_ratio)
        noise_indices = random.sample(range(len(df)), num_to_noise)
        
        for idx in noise_indices:
            current_score = df_noisy.iloc[idx]['score']
            
            # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º ¬±1 —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
            noise = random.choice([-1, 1])
            new_score = current_score + noise
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω
            new_score = max(-1, min(3, new_score))
            df_noisy.iloc[idx, df_noisy.columns.get_loc('score')] = new_score
        
        return df_noisy
    
    def create_hard_negatives(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã"""
        
        hard_negatives = []
        
        # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä—ã —Å –≤—ã—Å–æ–∫–∏–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏
        high_score_examples = df[df['score'] >= 2].copy()
        
        for _, row in high_score_examples.iterrows():
            # –°–æ–∑–¥–∞–µ–º –≤–µ—Ä—Å–∏—é —Å –ø–æ–Ω–∏–∂–µ–Ω–∏–µ–º –æ—Ü–µ–Ω–∫–∏
            hard_neg = row.copy()
            
            # –ò–∑–º–µ–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç, –¥–æ–±–∞–≤–ª—è—è –ø—Ä–æ–±–ª–µ–º—ã
            prompt = row['prompt']
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ –≤ "–æ—Ç–≤–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏"
            if "### –û—Ç–≤–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏:" in prompt:
                parts = prompt.split("### –û—Ç–≤–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏:")
                if len(parts) == 2:
                    answer_part = parts[1]
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                    error_modifications = [
                        lambda x: x + " (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç)",
                        lambda x: "–û—Ç–≤–µ—Ç: " + x,
                        lambda x: x + "\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ—è—Å–Ω–µ–Ω–∏—è...",
                        lambda x: x.upper(),  # –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–µ–≥–∏—Å—Ç—Ä
                        lambda x: x + " –∏ –µ—â–µ –Ω–µ–º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"
                    ]
                    
                    modification = random.choice(error_modifications)
                    modified_answer = modification(answer_part.strip())
                    
                    hard_neg['prompt'] = parts[0] + "### –û—Ç–≤–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏:\n" + modified_answer
                    hard_neg['score'] = max(0, row['score'] - 1)  # –ü–æ–Ω–∏–∂–∞–µ–º –æ—Ü–µ–Ω–∫—É
                    
                    hard_negatives.append(hard_neg)
        
        if hard_negatives:
            hard_neg_df = pd.DataFrame(hard_negatives)
            return pd.concat([df, hard_neg_df], ignore_index=True)
        
        return df


class AdvancedTrainingConfig:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.model_name = "Qwen/Qwen3-0.6B"
        self.max_seq_length = 3072
        self.load_in_4bit = True
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        self.per_device_train_batch_size = 2
        self.gradient_accumulation_steps = 8
        self.learning_rate = 5e-5
        self.num_train_epochs = 6
        self.warmup_ratio = 0.15
        self.weight_decay = 0.01
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã LoRA
        self.use_lora = True
        self.lora_r = 128  # –ë–æ–ª—å—à–æ–π rank –¥–ª—è –ª—É—á—à–µ–π –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.lora_alpha = 256
        self.lora_dropout = 0.05
        self.target_modules = [
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj",
            "lm_head"  # –î–æ–±–∞–≤–ª—è–µ–º –≥–æ–ª–æ–≤—É —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        ]
        
        # Regularization
        self.gradient_clipping = 1.0
        self.label_smoothing = 0.1
        self.drop_path_rate = 0.1
        
        # Curriculum learning
        self.curriculum_learning = True
        self.easy_examples_ratio = 0.3
        
        # Scheduling
        self.use_cosine_schedule = True
        self.min_lr_ratio = 0.1


def run_experiment_5_data_augmentation():
    """–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 5: –û–±—É—á–µ–Ω–∏–µ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π –¥–∞–Ω–Ω—ã—Ö"""
    
    print("\n" + "="*60)
    print("üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 5: Data Augmentation Training")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –∞—É–≥–º–µ–Ω—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    df = pd.read_csv("aij_judge_task_1_train.csv")
    print(f"–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    augmenter = DataAugmenter(seed=42)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é
    df_augmented = augmenter.paraphrase_prompts(df, augment_ratio=0.4)
    print(f"–ü–æ—Å–ª–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {len(df_augmented)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    df_augmented = augmenter.create_hard_negatives(df_augmented)
    print(f"–ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è hard negatives: {len(df_augmented)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    df_augmented.to_csv("aij_judge_augmented.csv", index=False)
    
    # –û–±—É—á–∞–µ–º —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    config = AdvancedTrainingConfig()
    
    from train_agent_judge import AgentJudgeTrainer
    trainer = AgentJudgeTrainer(config, "exp5_augmented_data")
    
    return trainer.train(
        train_path="aij_judge_augmented.csv",
        output_dir="models/exp5_augmented_data",
        balance_classes=True,
        balance_method="undersample"
    )


def run_experiment_6_curriculum_learning():
    """–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 6: Curriculum Learning"""
    
    print("\n" + "="*60)
    print("üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 6: Curriculum Learning")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = pd.read_csv("aij_judge_task_1_train.csv")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ—Ä–æ–≤
    def calculate_difficulty(prompt):
        """–ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"""
        difficulty_score = 0
        
        # –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞
        difficulty_score += len(prompt) / 1000
        
        # –ù–∞–ª–∏—á–∏–µ —Å–ª–æ–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        if "—Ñ—É–Ω–∫—Ü–∏" in prompt.lower():
            difficulty_score += 2
        if "–∫–æ–¥" in prompt.lower():
            difficulty_score += 1.5
        if "–º–∞—Ç–µ–º–∞—Ç–∏–∫" in prompt.lower():
            difficulty_score += 1
        if len(re.findall(r'\d+', prompt)) > 5:
            difficulty_score += 1
        
        return difficulty_score
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å
    df['difficulty'] = df['prompt'].apply(calculate_difficulty)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    df_sorted = df.sort_values('difficulty')
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —ç—Ç–∞–ø—ã curriculum
    total_size = len(df_sorted)
    stage1_size = int(total_size * 0.3)  # 30% –ª–µ–≥–∫–∏—Ö
    stage2_size = int(total_size * 0.5)  # 50% —Å—Ä–µ–¥–Ω–∏—Ö
    
    stage1_data = df_sorted[:stage1_size]
    stage2_data = df_sorted[:stage1_size + stage2_size]
    stage3_data = df_sorted  # –í—Å–µ –¥–∞–Ω–Ω—ã–µ
    
    print(f"Stage 1 (–ª–µ–≥–∫–∏–µ): {len(stage1_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"Stage 2 (—Å—Ä–µ–¥–Ω–∏–µ): {len(stage2_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"Stage 3 (–≤—Å–µ): {len(stage3_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç—Ç–∞–ø—ã
    stage1_data.to_csv("curriculum_stage1.csv", index=False)
    stage2_data.to_csv("curriculum_stage2.csv", index=False)
    stage3_data.to_csv("curriculum_stage3.csv", index=False)
    
    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å trainer –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ curriculum learning
    print("üí° Curriculum learning —Ç—Ä–µ–±—É–µ—Ç –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ trainer'–∞")
    print("–î–ª—è –¥–µ–º–æ –æ–±—É—á–∞–µ–º –Ω–∞ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å curriculum config")
    
    config = AdvancedTrainingConfig()
    config.curriculum_learning = True
    
    from train_agent_judge import AgentJudgeTrainer
    trainer = AgentJudgeTrainer(config, "exp6_curriculum")
    
    return trainer.train(
        train_path="curriculum_stage3.csv",
        output_dir="models/exp6_curriculum",
        balance_classes=False
    )


def run_experiment_7_ensemble():
    """–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 7: Ensemble –∏–∑ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    
    print("\n" + "="*60)  
    print("üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 7: Model Ensemble")
    print("="*60)
    
    # –û–±—É—á–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏
    configs = []
    
    # –ö–æ–Ω—Ñ–∏–≥ 1: –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π
    config1 = AdvancedTrainingConfig()
    config1.learning_rate = 1e-4
    config1.lora_r = 32
    config1.num_train_epochs = 4
    
    # –ö–æ–Ω—Ñ–∏–≥ 2: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π  
    config2 = AdvancedTrainingConfig()
    config2.learning_rate = 5e-4
    config2.lora_r = 256
    config2.num_train_epochs = 3
    
    # –ö–æ–Ω—Ñ–∏–≥ 3: –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
    config3 = AdvancedTrainingConfig()
    config3.learning_rate = 2e-4
    config3.lora_r = 64
    config3.num_train_epochs = 5
    
    configs = [
        (config1, "exp7_ensemble_conservative"),
        (config2, "exp7_ensemble_aggressive"), 
        (config3, "exp7_ensemble_balanced")
    ]
    
    results = {}
    
    for config, exp_name in configs:
        try:
            print(f"\nüîÑ –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å: {exp_name}")
            
            from train_agent_judge import AgentJudgeTrainer
            trainer = AgentJudgeTrainer(config, exp_name)
            
            result = trainer.train(
                train_path="aij_judge_task_1_train.csv",
                output_dir=f"models/{exp_name}",
                balance_classes=True,
                balance_method="undersample"
            )
            
            results[exp_name] = result
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ {exp_name}: {e}")
    
    print(f"\n‚úÖ –û–±—É—á–µ–Ω–æ {len(results)} –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è")
    return results


def create_ensemble_inference():
    """–°–æ–∑–¥–∞–µ—Ç —Å–∫—Ä–∏–ø—Ç –¥–ª—è ensemble inference"""
    
    ensemble_script = '''#!/usr/bin/env python3
"""
Ensemble Inference for Agent-as-Judge
====================================
"""

import pandas as pd
import numpy as np
from collections import Counter
import os

def ensemble_predict(test_path: str, pred_path: str):
    """–ê–Ω—Å–∞–º–±–ª–µ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
    
    # –ü—É—Ç–∏ –∫ –º–æ–¥–µ–ª—è–º –∞–Ω—Å–∞–º–±–ª—è
    ensemble_models = [
        "models/exp7_ensemble_conservative/unsloth_model",
        "models/exp7_ensemble_aggressive/unsloth_model", 
        "models/exp7_ensemble_balanced/unsloth_model"
    ]
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    all_predictions = []
    
    for model_path in ensemble_models:
        if os.path.exists(model_path):
            print(f"üîÆ –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç {model_path}")
            
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ–¥ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
            # predictions = get_predictions(test_path, model_path)
            # all_predictions.append(predictions)
    
    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –ø–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤—É)
    if all_predictions:
        final_predictions = []
        
        for i in range(len(all_predictions[0])):
            votes = [pred[i] for pred in all_predictions]
            majority_vote = Counter(votes).most_common(1)[0][0]
            final_predictions.append(majority_vote)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        test_df = pd.read_csv(test_path)
        result_df = pd.DataFrame({
            'id': test_df['id'],
            'score': final_predictions
        })
        
        result_df.to_csv(pred_path, index=False)
        print(f"üíæ Ensemble —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {pred_path}")

if __name__ == "__main__":
    ensemble_predict("test.csv", "ensemble_predictions.csv")
'''
    
    with open("ensemble_inference.py", "w", encoding="utf-8") as f:
        f.write(ensemble_script)
    
    print("üìù –°–æ–∑–¥–∞–Ω —Å–∫—Ä–∏–ø—Ç ensemble_inference.py")


def main():
    """–ó–∞–ø—É—Å–∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
    
    print("üöÄ –ó–ê–ü–£–°–ö –ü–†–û–î–í–ò–ù–£–¢–´–• –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í")
    print("="*60)
    
    experiments = [
        ("5", "Data Augmentation", run_experiment_5_data_augmentation),
        ("6", "Curriculum Learning", run_experiment_6_curriculum_learning), 
        ("7", "Model Ensemble", run_experiment_7_ensemble),
    ]
    
    results = {}
    
    for exp_id, exp_name, exp_func in experiments:
        try:
            print(f"\n‚ñ∂Ô∏è –ó–∞–ø—É—Å–∫–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç {exp_id}: {exp_name}")
            result = exp_func()
            results[exp_id] = result
            print(f"‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç {exp_id} –∑–∞–≤–µ—Ä—à–µ–Ω")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ {exp_id}: {e}")
            import traceback
            traceback.print_exc()
    
    # –°–æ–∑–¥–∞–µ–º ensemble –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
    create_ensemble_inference()
    
    print("\nüéØ –í—Å–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
    print(f"–£—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {len(results)} –∏–∑ {len(experiments)}")


if __name__ == "__main__":
    main()
