{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ‚öñÔ∏è Agent-as-Judge: Complete Training & Experiments\n",
        "\n",
        "–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏-—Å—É–¥—å–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤ LLM —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏ –∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.\n",
        "\n",
        "## üìã –ü–ª–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:\n",
        "1. **Baseline –æ–±—É—á–µ–Ω–∏–µ** - –ø—Ä–æ—Å—Ç–∞—è LoRA –Ω–∞—Å—Ç—Ä–æ–π–∫–∞\n",
        "2. **Balanced Classes** - —Ä–∞–±–æ—Ç–∞ —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤  \n",
        "3. **Data Augmentation** - —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "4. **Advanced Configuration** - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "5. **Model Evaluation** - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–¥—Ö–æ–¥–æ–≤\n",
        "6. **Final Export** - –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Installation & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏—é Python –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞–∫–µ—Ç—ã\n",
        "import sys\n",
        "import os\n",
        "print(f\"üêç Python –≤–µ—Ä—Å–∏—è: {sys.version}\")\n",
        "\n",
        "# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è Unsloth\n",
        "if \"COLAB_\" in \"\".join(os.environ.keys()):\n",
        "    # Colab installation\n",
        "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    %pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    %pip install --no-deps unsloth\n",
        "else:\n",
        "    # Local installation\n",
        "    %pip install unsloth\n",
        "\n",
        "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞–∫–µ—Ç—ã\n",
        "%pip install pandas numpy scikit-learn tqdm wandb matplotlib\n",
        "\n",
        "print(\"‚úÖ –í—Å–µ –ø–∞–∫–µ—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Unsloth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Unsloth\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "# Unsloth –∏–º–ø–æ—Ä—Ç—ã\n",
        "from unsloth import FastModel, is_bf16_supported\n",
        "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Unsloth –∏ –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Data Loading & Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "df = pd.read_csv('aij_judge_task_1_train.csv')\n",
        "\n",
        "print(f\"üìä –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(f\"üìã –ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}\")\n",
        "print(f\"\\nüìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫:\")\n",
        "score_dist = df['score'].value_counts().sort_index()\n",
        "print(score_dist)\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 6))\n",
        "score_dist.plot(kind='bar', color=['red', 'orange', 'yellow', 'green'])\n",
        "plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö')\n",
        "plt.xlabel('–û—Ü–µ–Ω–∫–∞')\n",
        "plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n",
        "\n",
        "# –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã –ø—Ä–æ–º–ø—Ç–æ–≤\n",
        "df['prompt_length'] = df['prompt'].str.len()\n",
        "print(f\"\\nüìè –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –ø—Ä–æ–º–ø—Ç–æ–≤:\")\n",
        "print(df['prompt_length'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö\n",
        "print(\"üîç –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "print(\"=\"*50)\n",
        "for i, row in df.head(2).iterrows():\n",
        "    print(f\"ID: {row['id']}\")\n",
        "    print(f\"Score: {row['score']}\")\n",
        "    print(f\"Prompt (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):\")\n",
        "    print(row['prompt'][:200] + \"...\" if len(row['prompt']) > 200 else row['prompt'])\n",
        "    print(\"-\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Data Augmentation (–∏–∑ Advanced Experiments)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataAugmenter:\n",
        "    \"\"\"–ö–ª–∞—Å—Å –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ experiments_advanced.py\"\"\"\n",
        "    \n",
        "    def __init__(self, seed: int = 42):\n",
        "        self.seed = seed\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "    \n",
        "    def paraphrase_prompts(self, df: pd.DataFrame, augment_ratio: float = 0.3) -> pd.DataFrame:\n",
        "        \"\"\"–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
        "        \n",
        "        paraphrase_templates = {\n",
        "            \"–∑–∞–¥–∞–Ω–∏–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏\": [\n",
        "                \"–ó–∞–¥–∞—á–∞ –¥–ª—è –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è\", \"–ü—Ä–∏–º–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\", \n",
        "                \"–°–ª—É—á–∞–π –¥–ª—è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è\", \"–ú–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è –æ—Ü–µ–Ω–∫–∏\"\n",
        "            ],\n",
        "            \"—ç—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç\": [\n",
        "                \"–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç\", \"–í–µ—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç\",\n",
        "                \"–û–±—Ä–∞–∑—Ü–æ–≤—ã–π –æ—Ç–≤–µ—Ç\", \"–ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç\"\n",
        "            ],\n",
        "            \"–æ—Ç–≤–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏\": [\n",
        "                \"–ü—Ä–æ–≤–µ—Ä—è–µ–º—ã–π –æ—Ç–≤–µ—Ç\", \"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–π –æ—Ç–≤–µ—Ç\",\n",
        "                \"–û—Ü–µ–Ω–∏–≤–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç\", \"–ò—Å—Å–ª–µ–¥—É–µ–º—ã–π –æ—Ç–≤–µ—Ç\"\n",
        "            ],\n",
        "            \"–∫—Ä–∏—Ç–µ—Ä–∏–π –æ—Ü–µ–Ω–∫–∏\": [\n",
        "                \"–ö—Ä–∏—Ç–µ—Ä–∏–π –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è\", \"–ü–∞—Ä–∞–º–µ—Ç—Ä –æ—Ü–µ–Ω–∫–∏\",\n",
        "                \"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞\", \"–ú–µ—Ä–∏–ª–æ –æ—Ü–µ–Ω–∫–∏\"\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        augmented_data = []\n",
        "        num_to_augment = int(len(df) * augment_ratio)\n",
        "        \n",
        "        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏\n",
        "        indices_to_augment = random.sample(range(len(df)), num_to_augment)\n",
        "        \n",
        "        for idx in indices_to_augment:\n",
        "            row = df.iloc[idx].copy()\n",
        "            prompt = row['prompt']\n",
        "            \n",
        "            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "            for original, variants in paraphrase_templates.items():\n",
        "                if original in prompt.lower():\n",
        "                    replacement = random.choice(variants)\n",
        "                    prompt = re.sub(\n",
        "                        re.escape(original), replacement, \n",
        "                        prompt, flags=re.IGNORECASE\n",
        "                    )\n",
        "            \n",
        "            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏\n",
        "            variations = [\n",
        "                (\"–í—ã–ø–æ–ª–Ω–∏\", \"–í—ã–ø–æ–ª–Ω–∏—Ç–µ\"), (\"–ù–∞–π–¥–∏\", \"–ù–∞–π–¥–∏—Ç–µ\"), \n",
        "                (\"–†–µ—à–∏\", \"–†–µ—à–∏—Ç–µ\"), (\"–û–ø—Ä–µ–¥–µ–ª–∏\", \"–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ\")\n",
        "            ]\n",
        "            \n",
        "            for old, new in variations:\n",
        "                if random.random() < 0.3:\n",
        "                    prompt = prompt.replace(old, new)\n",
        "            \n",
        "            row['prompt'] = prompt\n",
        "            augmented_data.append(row)\n",
        "        \n",
        "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
        "        augmented_df = pd.concat([df, pd.DataFrame(augmented_data)], ignore_index=True)\n",
        "        return augmented_df.sample(frac=1, random_state=self.seed)\n",
        "\n",
        "    def balance_classes(self, df: pd.DataFrame, method: str = \"undersample\") -> pd.DataFrame:\n",
        "        \"\"\"–ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å—ã –≤ –¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
        "        \n",
        "        if method == \"undersample\":\n",
        "            min_count = df['score'].value_counts().min()\n",
        "            balanced_dfs = []\n",
        "            \n",
        "            for score in df['score'].unique():\n",
        "                score_df = df[df['score'] == score]\n",
        "                if len(score_df) > min_count:\n",
        "                    score_df = score_df.sample(n=min_count, random_state=self.seed)\n",
        "                balanced_dfs.append(score_df)\n",
        "            \n",
        "            return pd.concat(balanced_dfs, ignore_index=True).sample(frac=1, random_state=self.seed)\n",
        "        \n",
        "        elif method == \"oversample\":\n",
        "            max_count = df['score'].value_counts().max()\n",
        "            balanced_dfs = []\n",
        "            \n",
        "            for score in df['score'].unique():\n",
        "                score_df = df[df['score'] == score]\n",
        "                if len(score_df) < max_count:\n",
        "                    additional_samples = max_count - len(score_df)\n",
        "                    extra_df = score_df.sample(n=additional_samples, replace=True, random_state=self.seed)\n",
        "                    score_df = pd.concat([score_df, extra_df], ignore_index=True)\n",
        "                balanced_dfs.append(score_df)\n",
        "            \n",
        "            return pd.concat(balanced_dfs, ignore_index=True).sample(frac=1, random_state=self.seed)\n",
        "        \n",
        "        return df\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –∞—É–≥–º–µ–Ω—Ç–µ—Ä–∞\n",
        "augmenter = DataAugmenter(seed=42)\n",
        "print(\"‚úÖ DataAugmenter —Å–æ–∑–¥–∞–Ω\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Model Setup (Unsloth + Fallback)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_model(model_name=\"Qwen/Qwen3-0.6B\", max_seq_length=1024, use_lora=True, lora_r=16):\n",
        "    \"\"\"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Unsloth\"\"\"\n",
        "    \n",
        "    print(f\"ü¶• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ Unsloth: {model_name}\")\n",
        "    \n",
        "    if use_lora:\n",
        "        # LoRA –æ–±—É—á–µ–Ω–∏–µ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)\n",
        "        model, tokenizer = FastModel.from_pretrained(\n",
        "            model_name=model_name,\n",
        "            max_seq_length=max_seq_length,\n",
        "            load_in_4bit=True,\n",
        "            dtype=None\n",
        "        )\n",
        "        \n",
        "        model = FastModel.get_peft_model(\n",
        "            model,\n",
        "            r=lora_r,\n",
        "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
        "                           \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "            lora_alpha=32,\n",
        "            lora_dropout=0.1,\n",
        "            bias=\"none\",\n",
        "            use_gradient_checkpointing=\"unsloth\",\n",
        "            random_state=42,\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ LoRA –º–æ–¥–µ–ª—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ —Å rank={lora_r}\")\n",
        "        \n",
        "    else:\n",
        "        # Full fine-tuning (—Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏)\n",
        "        model, tokenizer = FastModel.from_pretrained(\n",
        "            model_name=model_name,\n",
        "            max_seq_length=max_seq_length,\n",
        "            load_in_4bit=True,\n",
        "            dtype=None,\n",
        "            full_finetuning=True\n",
        "        )\n",
        "        \n",
        "        print(\"‚úÖ Full fine-tuning –º–æ–¥–µ–ª—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞\")\n",
        "    \n",
        "    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ chat template\n",
        "    tokenizer = get_chat_template(tokenizer, chat_template=\"qwen-3\")\n",
        "    \n",
        "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏\n",
        "    model.print_trainable_parameters() if hasattr(model, 'print_trainable_parameters') else None\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è setup_model –≥–æ—Ç–æ–≤–∞\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Data Preparation & Formatting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —á–∞—Ç —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\"\"\"\n",
        "    messages = [\n",
        "        [\n",
        "            {'role': 'user', 'content': prompt}, \n",
        "            {'role': 'assistant', 'content': str(int(score))}\n",
        "        ] \n",
        "        for prompt, score in zip(examples['prompt'], examples['score'])\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "def prepare_dataset(df, tokenizer, test_size=0.2):\n",
        "    \"\"\"–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\"\"\"\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–µ–º Dataset –∏–∑ pandas\n",
        "    dataset = Dataset.from_pandas(df[['prompt', 'score']])\n",
        "    \n",
        "    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç—ã\n",
        "    def format_chat(examples):\n",
        "        messages = formatting_prompts_func(examples)\n",
        "        texts = []\n",
        "        \n",
        "        for message in messages:\n",
        "            formatted = tokenizer.apply_chat_template(\n",
        "                message, \n",
        "                tokenize=False, \n",
        "                add_generation_prompt=False\n",
        "            )\n",
        "            texts.append(formatted)\n",
        "        \n",
        "        return {\"text\": texts}\n",
        "    \n",
        "    dataset = dataset.map(format_chat, batched=True)\n",
        "    \n",
        "    # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–º–µ—Ä –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    print(\"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    sample_text = dataset['text'][0]\n",
        "    print(f\"–ü—Ä–∏–º–µ—Ä –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\\n{repr(sample_text)}\\n\")\n",
        "    \n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π\n",
        "    if '<|im_start|>user\\n' in sample_text:\n",
        "        print(\"‚úÖ –ù–∞–π–¥–µ–Ω —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å '<|im_start|>user\\\\n'\")\n",
        "    else:\n",
        "        print(\"‚ùå –ù–ï –Ω–∞–π–¥–µ–Ω —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å '<|im_start|>user\\\\n'\")\n",
        "        \n",
        "    if '<|im_start|>assistant\\n' in sample_text:\n",
        "        print(\"‚úÖ –ù–∞–π–¥–µ–Ω —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å '<|im_start|>assistant\\\\n'\")\n",
        "    else:\n",
        "        print(\"‚ùå –ù–ï –Ω–∞–π–¥–µ–Ω —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å '<|im_start|>assistant\\\\n'\")\n",
        "    \n",
        "    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ train/test\n",
        "    dataset = dataset.train_test_split(test_size=test_size, seed=42)\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def prepare_dataset_manual(df, tokenizer, test_size=0.2):\n",
        "    \"\"\"–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Ä—É—á–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º\"\"\"\n",
        "    print(\"üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—á–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å train_on_responses_only\")\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–µ–º Dataset –∏–∑ pandas\n",
        "    dataset = Dataset.from_pandas(df[['prompt', 'score']])\n",
        "    \n",
        "    # –†—É—á–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Å—Ç–∏–ª–µ Qwen\n",
        "    def format_manually(examples):\n",
        "        texts = []\n",
        "        for prompt, score in zip(examples['prompt'], examples['score']):\n",
        "            # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç –≤ —Ç–æ—á–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –∫–æ—Ç–æ—Ä—ã–π –æ–∂–∏–¥–∞–µ—Ç train_on_responses_only\n",
        "            text = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n{int(score)}<|im_end|>\\n\"\n",
        "            texts.append(text)\n",
        "        return {\"text\": texts}\n",
        "    \n",
        "    dataset = dataset.map(format_manually, batched=True)\n",
        "    \n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "    print(\"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä—É—á–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:\")\n",
        "    sample_text = dataset['text'][0]\n",
        "    print(f\"–ü—Ä–∏–º–µ—Ä:\\n{repr(sample_text)}\\n\")\n",
        "    \n",
        "    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ train/test\n",
        "    dataset = dataset.train_test_split(test_size=test_size, seed=42)\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "print(\"‚úÖ –§—É–Ω–∫—Ü–∏–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≥–æ—Ç–æ–≤—ã\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1: Baseline LoRA Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1: Baseline –æ–±—É—á–µ–Ω–∏–µ\n",
        "print(\"üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 1: Baseline LoRA Training\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –î–ê–ù–ù–´–• –ü–ï–†–ï–î –û–ë–£–ß–ï–ù–ò–ï–ú\n",
        "print(\"\\nüîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç...\")\n",
        "diagnose_dataset(dataset, tokenizer, num_samples=2)\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–ª—è train_on_responses_only\n",
        "user_sep, assistant_sep = test_train_on_responses_only_format(dataset, tokenizer)\n",
        "\n",
        "if user_sep is None:\n",
        "    print(\"‚ùå –ü–†–û–ë–õ–ï–ú–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏!\")\n",
        "    print(\"üí° –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Ä—É—á–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "    \n",
        "    # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å —Ä—É—á–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º\n",
        "    dataset = prepare_dataset_manual(df, tokenizer)\n",
        "    \n",
        "    print(\"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤—Ä—É—á–Ω—É—é!\")\n",
        "    \n",
        "    # –ü–æ–≤—Ç–æ—Ä–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º\n",
        "    user_sep, assistant_sep = test_train_on_responses_only_format(dataset, tokenizer)\n",
        "    \n",
        "    if user_sep is not None:\n",
        "        print(f\"‚úÖ –¢–µ–ø–µ—Ä—å –Ω–∞–π–¥–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏: '{user_sep}' –∏ '{assistant_sep}'\")\n",
        "    else:\n",
        "        print(\"‚ùå –ü—Ä–æ–±–ª–µ–º–∞ –≤—Å–µ –µ—â–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –í–æ–∑–º–æ–∂–Ω–æ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å chat template.\")\n",
        "else:\n",
        "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏: '{user_sep}' –∏ '{assistant_sep}'\")\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "model, tokenizer = setup_model(\n",
        "    model_name=\"Qwen/Qwen3-0.6B\",\n",
        "    max_seq_length=1024,\n",
        "    use_lora=True,\n",
        "    lora_r=16\n",
        ")\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–±–æ–ª—å—à—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è –¥–µ–º–æ)\n",
        "sample_df = df.sample(n=1000, random_state=42)  # –ú–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å .sample() –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "dataset = prepare_dataset(sample_df, tokenizer)\n",
        "\n",
        "print(f\"üìä –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(dataset['train'])} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(f\"üìä –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(dataset['test'])} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
        "print(\"\\\\nüîç –ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\")\n",
        "print(dataset['train'][0]['text'][:500] + \"...\" if len(dataset['train'][0]['text']) > 500 else dataset['train'][0]['text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –¥–ª—è Baseline —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
        "from trl import SFTConfig\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    args=SFTConfig(\n",
        "        dataset_text_field=\"text\",\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_ratio=0.05,\n",
        "        num_train_epochs=2,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –¥–µ–º–æ\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=5,\n",
        "        optim=\"adamw_torch\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=42,\n",
        "        output_dir=\"./baseline_model\",\n",
        "        # –£–±–∏—Ä–∞–µ–º evaluation_strategy - –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö SFTConfig\n",
        "        # evaluation_strategy=\"steps\",\n",
        "        # eval_steps=50,\n",
        "        save_steps=100,\n",
        "        save_total_limit=2,\n",
        "        report_to=\"none\",  # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞ \"wandb\" –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
        "    ),\n",
        ")\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Ç–≤–µ—Ç–∞—Ö (–Ω–µ –Ω–∞ –ø—Ä–æ–º–ø—Ç–∞—Ö)\n",
        "# –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –∏ –æ—Ç–∫–ª—é—á–∞–µ–º –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part=\"<|im_start|>user\\n\",\n",
        "    response_part=\"<|im_start|>assistant\\n\",\n",
        "    num_proc=1  # –û—Ç–∫–ª—é—á–∞–µ–º –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è Baseline —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\")\n",
        "\n",
        "# üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ train_on_responses_only –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø—Ä–∏–º–µ–Ω–∏–ª—Å—è\n",
        "print(\"üß™ –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ train_on_responses_only...\")\n",
        "try:\n",
        "    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∏–∑ dataloader'–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ª–µ–π–±–ª–æ–≤\n",
        "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
        "    labels = sample_batch['labels']\n",
        "    non_masked_labels = (labels != -100).sum().item()\n",
        "    total_labels = labels.numel()\n",
        "    \n",
        "    print(f\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ª–µ–π–±–ª–æ–≤:\")\n",
        "    print(f\"   –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_labels}\")\n",
        "    print(f\"   –ù–µ –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö: {non_masked_labels}\")\n",
        "    print(f\"   –ü—Ä–æ—Ü–µ–Ω—Ç –æ–±—É—á–∞–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {non_masked_labels/total_labels*100:.1f}%\")\n",
        "    \n",
        "    if non_masked_labels == 0:\n",
        "        print(\"‚ùå –ü–†–û–ë–õ–ï–ú–ê: –í—Å–µ –ª–µ–π–±–ª—ã –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω—ã (-100)!\")\n",
        "        print(\"üí° –ù—É–∂–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –≤ train_on_responses_only\")\n",
        "    else:\n",
        "        print(\"‚úÖ –õ–µ–π–±–ª—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–µ–π–±–ª—ã: {e}\")\n",
        "    print(\"–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "\n",
        "# –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø\n",
        "print(\"\\\\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º Baseline –æ–±—É—á–µ–Ω–∏–µ...\")\n",
        "baseline_stats = trainer.train()\n",
        "\n",
        "print(\"\\\\n‚úÖ Baseline –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 2: Balanced Classes Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 2: –û–±—É—á–µ–Ω–∏–µ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤\n",
        "print(\"üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 2: Balanced Classes Training\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
        "print(\"‚öñÔ∏è –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º –∫–ª–∞—Å—Å—ã...\")\n",
        "balanced_df = augmenter.balance_classes(df, method=\"undersample\")\n",
        "\n",
        "print(\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏:\")\n",
        "print(df['score'].value_counts().sort_index())\n",
        "print(\"\\\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏:\")\n",
        "print(balanced_df['score'].value_counts().sort_index())\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ 2\n",
        "model2, tokenizer2 = setup_model(\n",
        "    model_name=\"Qwen/Qwen3-0.6B\",\n",
        "    max_seq_length=1024,\n",
        "    use_lora=True,\n",
        "    lora_r=32  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º rank –¥–ª—è –ª—É—á—à–µ–π –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        ")\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "dataset2 = prepare_dataset(balanced_df, tokenizer2)\n",
        "\n",
        "print(f\"üìä –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(dataset2['train'])} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(f\"üìä –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(dataset2['test'])} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "trainer2 = SFTTrainer(\n",
        "    model=model2,\n",
        "    tokenizer=tokenizer2,\n",
        "    train_dataset=dataset2['train'],\n",
        "    eval_dataset=dataset2['test'],\n",
        "    args=SFTConfig(\n",
        "        dataset_text_field=\"text\",\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "        warmup_ratio=0.1,\n",
        "        num_train_epochs=3,  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "        learning_rate=1.5e-4,  # –ß—É—Ç—å –º–µ–Ω—å—à–∏–π learning rate\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_torch\",\n",
        "        weight_decay=0.005,  # –ú–µ–Ω—å—à–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=42,\n",
        "        output_dir=\"./balanced_model\",\n",
        "        # –£–±–∏—Ä–∞–µ–º evaluation_strategy - –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö SFTConfig\n",
        "        # evaluation_strategy=\"steps\",\n",
        "        # eval_steps=50,\n",
        "        save_steps=100,\n",
        "        save_total_limit=2,\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Ç–≤–µ—Ç–∞—Ö\n",
        "# –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –∏ –æ—Ç–∫–ª—é—á–∞–µ–º –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥  \n",
        "trainer2 = train_on_responses_only(\n",
        "    trainer2,\n",
        "    instruction_part=\"<|im_start|>user\\n\",\n",
        "    response_part=\"<|im_start|>assistant\\n\",\n",
        "    num_proc=1  # –û—Ç–∫–ª—é—á–∞–µ–º –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫\n",
        ")\n",
        "\n",
        "# –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø\n",
        "print(\"\\\\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤...\")\n",
        "balanced_stats = trainer2.train()\n",
        "\n",
        "print(\"\\\\n‚úÖ Balanced –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 3: Data Augmentation Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 3: Data Augmentation\n",
        "print(\"üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 3: Data Augmentation Training\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é\n",
        "print(\"üîÑ –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "augmented_df = augmenter.paraphrase_prompts(df, augment_ratio=0.4)\n",
        "\n",
        "print(f\"–†–∞–∑–º–µ—Ä –¥–æ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(f\"–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {len(augmented_df)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(f\"–î–æ–±–∞–≤–ª–µ–Ω–æ: {len(augmented_df) - len(df)} –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "\n",
        "# –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "augmented_balanced_df = augmenter.balance_classes(augmented_df, method=\"undersample\")\n",
        "\n",
        "print(f\"\\\\n–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {len(augmented_balanced_df)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(\"\\\\n–ò—Ç–æ–≥–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\")\n",
        "print(augmented_balanced_df['score'].value_counts().sort_index())\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ 3\n",
        "model3, tokenizer3 = setup_model(\n",
        "    model_name=\"Qwen/Qwen3-0.6B\",\n",
        "    max_seq_length=1536,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤\n",
        "    use_lora=True,\n",
        "    lora_r=64  # –ë–æ–ª—å—à–æ–π rank –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
        ")\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "dataset3 = prepare_dataset(augmented_balanced_df, tokenizer3)\n",
        "\n",
        "print(f\"üìä –ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(dataset3['train'])} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(f\"üìä –ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(dataset3['test'])} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 3: Augmented Training\n",
        "\n",
        "**–¶–µ–ª—å:** –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º.\n",
        "\n",
        "**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**\n",
        "- üîÑ **–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö**: –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤\n",
        "- ‚öñÔ∏è **–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤**: –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫  \n",
        "- üìä **–£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –æ–±—ä–µ–º**: –ë–æ–ª—å—à–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "- üéØ **–ü–æ–≤—ã—à–µ–Ω–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å**: –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π LoRA rank (64) –∏ –±–æ–ª—å—à–µ —ç–ø–æ—Ö (4)\n",
        "\n",
        "**–û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:**\n",
        "- ‚úÖ –õ—É—á—à–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–∞—Ö\n",
        "- ‚úÖ –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ –≤—Å–µ–º –∫–ª–∞—Å—Å–∞–º\n",
        "- ‚úÖ –ü–æ–≤—ã—à–µ–Ω–Ω–∞—è —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 3: –û–±—É—á–µ–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö \n",
        "trainer3 = SFTTrainer(\n",
        "    model=model3,\n",
        "    tokenizer=tokenizer3,\n",
        "    train_dataset=dataset3['train'],\n",
        "    eval_dataset=dataset3['test'],\n",
        "    args=SFTConfig(\n",
        "        dataset_text_field=\"text\",\n",
        "        per_device_train_batch_size=1,  # –ú–µ–Ω—å—à–∏–π batch –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "        gradient_accumulation_steps=8,  # –ë–æ–ª—å—à–µ accumulation –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "        warmup_ratio=0.15,  # –ë–æ–ª—å—à–µ warmup –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "        num_train_epochs=4,  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "        learning_rate=1e-4,  # –ï—â–µ –º–µ–Ω—å—à–∏–π learning rate\n",
        "        logging_steps=20,\n",
        "        optim=\"adamw_torch\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=42,\n",
        "        output_dir=\"./augmented_model\",\n",
        "        save_steps=150,\n",
        "        save_total_limit=2,\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Ç–≤–µ—Ç–∞—Ö - —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏\n",
        "trainer3 = train_on_responses_only(\n",
        "    trainer3,\n",
        "    instruction_part=\"<|im_start|>user\\n\",\n",
        "    response_part=\"<|im_start|>assistant\\n\", \n",
        "    num_proc=1  # –û—Ç–∫–ª—é—á–∞–µ–º –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è Augmented —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\")\n",
        "\n",
        "# –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê 3\n",
        "print(\"\\\\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "augmented_stats = trainer3.train()\n",
        "\n",
        "print(\"\\\\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÆ Model Inference & Testing (Compatible with run.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å run.py\n",
        "print(\"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏...\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å\n",
        "model.save_pretrained(\"./aij_qwen_0.6b_baseline\")\n",
        "tokenizer.save_pretrained(\"./aij_qwen_0.6b_baseline\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å (–æ–±—ã—á–Ω–æ –ª—É—á—à–µ –≤—Å–µ–≥–æ)\n",
        "model2.save_pretrained(\"./aij_qwen_0.6b\")  # –≠—Ç–æ –∏–º—è –æ–∂–∏–¥–∞–µ—Ç run.py\n",
        "tokenizer2.save_pretrained(\"./aij_qwen_0.6b\")\n",
        "\n",
        "print(\"‚úÖ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–Ω—Ñ–µ—Ä–µ–Ω—Å —Ñ—É–Ω–∫—Ü–∏—è (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è run.py)\n",
        "def run_inference(model, tokenizer, test_prompts, max_tokens=10, temperature=0.0):\n",
        "    \"\"\"–ó–∞–ø—É—Å–∫–∞–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–∫–∞–∫ –≤ run.py)\"\"\"\n",
        "    \n",
        "    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç—ã –≤ —á–∞—Ç —Ñ–æ—Ä–º–∞—Ç\n",
        "    formatted_prompts = []\n",
        "    for prompt in test_prompts:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        formatted = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "            enable_thinking=False\n",
        "        )\n",
        "        formatted_prompts.append(formatted)\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã –±–∞—Ç—á–∞–º–∏\n",
        "    batch_size = 4\n",
        "    for i in range(0, len(formatted_prompts), batch_size):\n",
        "        batch_prompts = formatted_prompts[i:i+batch_size]\n",
        "        \n",
        "        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "        inputs = tokenizer(\n",
        "            batch_prompts, \n",
        "            return_tensors=\"pt\", \n",
        "            padding=True, \n",
        "            truncation=True,\n",
        "            max_length=1536\n",
        "        )\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "        \n",
        "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "        for j, output in enumerate(outputs):\n",
        "            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
        "            new_tokens = output[inputs['input_ids'][j].shape[0]:]\n",
        "            generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "            \n",
        "            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É (–∫–∞–∫ –≤ run.py)\n",
        "            score = int(generated_text[0]) if generated_text and generated_text[0].isdigit() else 0\n",
        "            results.append(score)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
        "print(\"üîÆ –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å...\")\n",
        "test_sample = df.head(10)\n",
        "test_predictions = run_inference(model2, tokenizer2, test_sample['prompt'].tolist())\n",
        "\n",
        "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞:\")\n",
        "for i, (true_score, pred_score, prompt_preview) in enumerate(zip(\n",
        "    test_sample['score'].tolist(),\n",
        "    test_predictions,\n",
        "    [p[:100] + \"...\" for p in test_sample['prompt'].tolist()]\n",
        ")):\n",
        "    print(f\"–ü—Ä–∏–º–µ—Ä {i+1}: True={true_score}, Pred={pred_score}, Prompt='{prompt_preview}'\")\n",
        "\n",
        "# –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞\n",
        "accuracy = accuracy_score(test_sample['score'].tolist(), test_predictions)\n",
        "print(f\"\\\\nüìä –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {accuracy:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å run.py\n",
        "\n",
        "–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —ç—Ç–æ–≥–æ notebook, –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫–µ `aij_qwen_0.6b` –∏ –≥–æ—Ç–æ–≤–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å `run.py`.\n",
        "\n",
        "### –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:\n",
        "1. **–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞** - –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø–∞–ø–∫–∞ `aij_qwen_0.6b/`\n",
        "2. **–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ** –≤ CSV —Ñ–æ—Ä–º–∞—Ç–µ —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ `id` –∏ `prompt` \n",
        "3. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ inference**:\n",
        "   ```bash\n",
        "   python run.py --test_path test.csv --pred_path predictions.csv\n",
        "   ```\n",
        "\n",
        "### –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏ run.py:\n",
        "- –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ `aij_qwen_0.6b/`  \n",
        "- –ü—Ä–∏–º–µ–Ω—è–µ—Ç chat template –∫ –ø—Ä–æ–º–ø—Ç–∞–º\n",
        "- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã (max_tokens=10)\n",
        "- –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–≤—É—é —Ü–∏—Ñ—Ä—É –∫–∞–∫ –æ—Ü–µ–Ω–∫—É\n",
        "- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV\n",
        "\n",
        "### –ù–∞—à–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏:\n",
        "- **Baseline LoRA**: –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
        "- **Balanced Classes**: –£–ª—É—á—à–µ–Ω–Ω—ã–π F1-score –∑–∞ —Å—á–µ—Ç –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏  \n",
        "- **Data Augmentation**: –ü–æ–≤—ã—à–µ–Ω–Ω–∞—è robustness\n",
        "\n",
        "–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å (–æ–±—ã—á–Ω–æ Balanced Classes) —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ `aij_qwen_0.6b` –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n",
        "\n",
        "### –ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ —É–ª—É—á—à–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–∞–ª—å—à–µ:\n",
        "\n",
        "1. **–£–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏**: \n",
        "   - –ü–æ–ø—Ä–æ–±—É–π—Ç–µ `Qwen/Qwen3-1.5B` –∏–ª–∏ `Qwen/Qwen3-7B`\n",
        "   \n",
        "2. **–ë–æ–ª—å—à–µ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è**:\n",
        "   - –£–≤–µ–ª–∏—á—å—Ç–µ `num_train_epochs` –¥–æ 5-10\n",
        "   \n",
        "3. **Hyperparameter tuning**:\n",
        "   - –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å `learning_rate`, `lora_r`, `lora_alpha`\n",
        "   \n",
        "4. **Ensemble –º–µ—Ç–æ–¥—ã**:\n",
        "   - –û–±—É—á–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –∏ —É—Å—Ä–µ–¥–Ω–∏—Ç–µ –∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "\n",
        "5. **Advanced augmentation**:\n",
        "   - Back-translation —á–µ—Ä–µ–∑ –¥—Ä—É–≥–∏–µ —è–∑—ã–∫–∏\n",
        "   - Synthetic error injection\n",
        "   \n",
        "### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞:\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `report_to=\"wandb\"` –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
        "- –î–æ–±–∞–≤—å—Ç–µ –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (F1, confusion matrix)\n",
        "- –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ hold-out —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
        "\n",
        "**–£–¥–∞—á–∏ –≤ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–∏! üèÜ**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π TRL\n",
        "\n",
        "**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ**: –í –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ `trl` –∏–∑–º–µ–Ω–∏–ª—Å—è API –¥–ª—è `SFTConfig`. \n",
        "\n",
        "### –û—Å–Ω–æ–≤–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è:\n",
        "- ‚ùå `evaluation_strategy` –±–æ–ª—å—à–µ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ `SFTConfig`\n",
        "- ‚ùå `eval_steps` —Ç–∞–∫–∂–µ —É–±—Ä–∞–Ω –∏–∑ `SFTConfig` \n",
        "- ‚úÖ –≠—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–µ–ø–µ—Ä—å —É–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\n",
        "\n",
        "### –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –æ—Ü–µ–Ω–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:\n",
        "–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã:\n",
        "1. –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π —Ä—É—á–Ω–æ–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å\n",
        "2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–ª–±—ç–∫–æ–≤\n",
        "3. –û—Ç–¥–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
        "\n",
        "–¢–µ–∫—É—â–∏–π –∫–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ evaluation - –º–æ–¥–µ–ª—å –ø—Ä–æ—Å—Ç–æ –æ–±—É—á–∞–µ—Ç—Å—è –±–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ train_on_responses_only\n",
        "def diagnose_dataset(dataset, tokenizer, num_samples=3):\n",
        "    \"\"\"–î–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è train_on_responses_only\"\"\"\n",
        "    print(\"üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –î–ê–¢–ê–°–ï–¢–ê\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö\n",
        "    for i in range(min(num_samples, len(dataset['train']))):\n",
        "        example = dataset['train'][i]\n",
        "        text = example['text']\n",
        "        \n",
        "        print(f\"\\nüìÑ –ü—Ä–∏–º–µ—Ä {i+1}:\")\n",
        "        print(f\"–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: {repr(text[:200])}\")\n",
        "        \n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π\n",
        "        if '<|im_start|>user' in text:\n",
        "            print(\"‚úÖ –ù–∞–π–¥–µ–Ω —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\")\n",
        "        else:\n",
        "            print(\"‚ùå –ù–ï –Ω–∞–π–¥–µ–Ω —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\")\n",
        "            \n",
        "        if '<|im_start|>assistant' in text:\n",
        "            print(\"‚úÖ –ù–∞–π–¥–µ–Ω —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞\")\n",
        "        else:\n",
        "            print(\"‚ùå –ù–ï –Ω–∞–π–¥–µ–Ω —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞\")\n",
        "            \n",
        "        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º\n",
        "        tokens = tokenizer(text, return_tensors=\"pt\")\n",
        "        print(f\"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(tokens.input_ids[0])}\")\n",
        "        \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "def test_train_on_responses_only_format(dataset, tokenizer):\n",
        "    \"\"\"–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º–∞—Ç–∞ –¥–ª—è train_on_responses_only\"\"\"\n",
        "    print(\"üß™ –¢–ï–°–¢ train_on_responses_only –§–û–†–ú–ê–¢–ê\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    example_text = dataset['train'][0]['text']\n",
        "    print(f\"–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞:\\n{example_text}\\n\")\n",
        "    \n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π\n",
        "    separators = [\n",
        "        (\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n",
        "        (\"<|im_start|>user\\\\n\", \"<|im_start|>assistant\\\\n\"),\n",
        "        (\"user\\n\", \"assistant\\n\"),\n",
        "        (\"user\", \"assistant\")\n",
        "    ]\n",
        "    \n",
        "    for user_sep, assistant_sep in separators:\n",
        "        print(f\"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏: '{user_sep}' –∏ '{assistant_sep}'\")\n",
        "        if user_sep in example_text and assistant_sep in example_text:\n",
        "            print(\"‚úÖ –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –Ω–∞–π–¥–µ–Ω—ã!\")\n",
        "            \n",
        "            # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —á–∞—Å—Ç–∏\n",
        "            try:\n",
        "                user_part = example_text.split(user_sep)[1].split(assistant_sep)[0]\n",
        "                assistant_part = example_text.split(assistant_sep)[1]\n",
        "                print(f\"üë§ User: {repr(user_part[:100])}\")\n",
        "                print(f\"ü§ñ Assistant: {repr(assistant_part[:50])}\")\n",
        "                return user_sep, assistant_sep\n",
        "            except:\n",
        "                print(\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ\")\n",
        "        else:\n",
        "            print(\"‚ùå –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")\n",
        "    \n",
        "    return None, None\n",
        "\n",
        "# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
        "def evaluate_model_manually(model, tokenizer, eval_dataset, num_samples=50):\n",
        "    \"\"\"–†—É—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
        "    \n",
        "    # –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏\n",
        "    eval_sample = eval_dataset.shuffle(seed=42).select(range(min(num_samples, len(eval_dataset))))\n",
        "    \n",
        "    # –ü—Ä–æ—Å—Ç–æ–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for example in eval_sample:\n",
        "        prompt = example['text'].split('<|im_start|>user\\n')[1].split('<|im_start|>assistant\\n')[0]\n",
        "        true_score = int(example['text'].split('<|im_start|>assistant\\n')[1].strip())\n",
        "        \n",
        "        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        inputs = tokenizer(formatted, return_tensors=\"pt\")\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=5, temperature=0.0)\n",
        "            \n",
        "        generated = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
        "        pred_score = int(generated[0]) if generated and generated[0].isdigit() else 0\n",
        "        \n",
        "        if pred_score == true_score:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    print(f\"üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏: {correct}/{total} = {accuracy:.3f}\")\n",
        "    return accuracy\n",
        "\n",
        "print(\"‚úÖ –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≥–æ—Ç–æ–≤—ã\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
