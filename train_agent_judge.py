#!/usr/bin/env python3
"""
Agent-as-Judge Model Training Script
====================================

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç —Ä–µ–∞–ª–∏–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏-—Å—É–¥—å–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤ LLM –º–æ–¥–µ–ª–µ–π.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—É—á–µ–Ω–∏—è –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã.
"""

import os
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
import torch
import wandb
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report
import argparse
from pathlib import Path
import json
from datetime import datetime

# Unsloth imports
from unsloth import FastModel, is_bf16_supported
from unsloth.chat_templates import get_chat_template
from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq
import warnings
warnings.filterwarnings("ignore")


class AgentJudgeConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Agent-as-Judge –º–æ–¥–µ–ª–∏"""
    
    def __init__(self):
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        self.model_name = "Qwen/Qwen3-0.6B"
        self.max_seq_length = 2048  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
        self.load_in_4bit = True    # –î–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        self.load_in_8bit = False
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        self.per_device_train_batch_size = 4
        self.per_device_eval_batch_size = 8
        self.gradient_accumulation_steps = 4
        self.warmup_ratio = 0.05
        self.num_train_epochs = 3
        self.learning_rate = 2e-4
        self.weight_decay = 0.01
        self.lr_scheduler_type = "cosine"
        self.logging_steps = 25
        self.eval_strategy = "steps"
        self.eval_steps = 200
        self.save_strategy = "steps" 
        self.save_steps = 500
        self.save_total_limit = 2
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã LoRA (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
        self.use_lora = True
        self.lora_r = 16
        self.lora_alpha = 32
        self.lora_dropout = 0.1
        self.target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", 
                              "gate_proj", "up_proj", "down_proj"]
        
        # –ü—Ä–æ—á–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.seed = 42
        self.fp16 = not is_bf16_supported()
        self.bf16 = is_bf16_supported()
        self.gradient_checkpointing = True
        self.dataloader_num_workers = 2


class DataPreprocessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, tokenizer, config: AgentJudgeConfig):
        self.tokenizer = tokenizer
        self.config = config
        
    def format_prompt(self, example: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        input_prompt = example['prompt']
        score = example['score']
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –¥–ª—è –º–æ–¥–µ–ª–∏
        system_prompt = """–¢—ã - –º–æ–¥–µ–ª—å-—Å—É–¥—å—è, –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –≤—ã—Å—Ç–∞–≤—å –æ—Ü–µ–Ω–∫—É —Å–æ–≥–ª–∞—Å–Ω–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é –∏ —à–∫–∞–ª–µ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è. –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–º –æ—Ç 0 –¥–æ 3 (–∏–ª–∏ -1, –µ—Å–ª–∏ –∫—Ä–∏—Ç–µ—Ä–∏–π –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º)."""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∏–∞–ª–æ–≥
        conversation = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": input_prompt},
            {"role": "assistant", "content": str(score)}
        ]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º chat template
        formatted = self.tokenizer.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=False
        )
        
        return formatted
    
    def prepare_dataset(self, df: pd.DataFrame) -> Dataset:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        formatted_data = []
        for _, row in df.iterrows():
            formatted_text = self.format_prompt(row.to_dict())
            formatted_data.append({"text": formatted_text})
        
        return Dataset.from_list(formatted_data)
    
    def split_data(self, df: pd.DataFrame, test_size: float = 0.15, 
                   stratify: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏"""
        
        if stratify:
            # –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º
            train_df, val_df = train_test_split(
                df, test_size=test_size, 
                stratify=df['score'], 
                random_state=self.config.seed
            )
        else:
            train_df, val_df = train_test_split(
                df, test_size=test_size, 
                random_state=self.config.seed
            )
        
        return train_df, val_df
    
    def balance_classes(self, df: pd.DataFrame, method: str = "undersample") -> pd.DataFrame:
        """–ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å—ã –≤ –¥–∞–Ω–Ω—ã—Ö"""
        
        if method == "undersample":
            # –ü–æ–¥–≤—ã–±–æ—Ä–∫–∞ –¥–æ —Ä–∞–∑–º–µ—Ä–∞ –Ω–∞–∏–º–µ–Ω—å—à–µ–≥–æ –∫–ª–∞—Å—Å–∞
            min_count = df['score'].value_counts().min()
            balanced_dfs = []
            
            for score in df['score'].unique():
                score_df = df[df['score'] == score]
                if len(score_df) > min_count:
                    score_df = score_df.sample(n=min_count, random_state=self.config.seed)
                balanced_dfs.append(score_df)
            
            return pd.concat(balanced_dfs, ignore_index=True).sample(
                frac=1, random_state=self.config.seed
            )
        
        elif method == "oversample":
            # –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ —Ä–∞–∑–º–µ—Ä–∞ –Ω–∞–∏–±–æ–ª—å—à–µ–≥–æ –∫–ª–∞—Å—Å–∞
            max_count = df['score'].value_counts().max()
            balanced_dfs = []
            
            for score in df['score'].unique():
                score_df = df[df['score'] == score]
                if len(score_df) < max_count:
                    # –î—É–±–ª–∏—Ä—É–µ–º —Å –∑–∞–º–µ–Ω–æ–π
                    additional_samples = max_count - len(score_df)
                    extra_df = score_df.sample(
                        n=additional_samples, 
                        replace=True, 
                        random_state=self.config.seed
                    )
                    score_df = pd.concat([score_df, extra_df], ignore_index=True)
                balanced_dfs.append(score_df)
            
            return pd.concat(balanced_dfs, ignore_index=True).sample(
                frac=1, random_state=self.config.seed
            )
        
        else:
            return df


class AgentJudgeTrainer:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏-—Å—É–¥—å–∏"""
    
    def __init__(self, config: AgentJudgeConfig, experiment_name: str = "agent_judge_v1"):
        self.config = config
        self.experiment_name = experiment_name
        self.model = None
        self.tokenizer = None
        self.trainer = None
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        torch.manual_seed(config.seed)
        np.random.seed(config.seed)
        
    def setup_model_and_tokenizer(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""
        
        print(f"ü§ñ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {self.config.model_name}...")
        
        if self.config.use_lora:
            # LoRA –æ–±—É—á–µ–Ω–∏–µ
            self.model, self.tokenizer = FastModel.from_pretrained(
                model_name=self.config.model_name,
                max_seq_length=self.config.max_seq_length,
                load_in_4bit=self.config.load_in_4bit,
                load_in_8bit=self.config.load_in_8bit,
                dtype=torch.bfloat16 if self.config.bf16 else torch.float16,
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã
            self.model = FastModel.get_peft_model(
                self.model,
                r=self.config.lora_r,
                target_modules=self.config.target_modules,
                lora_alpha=self.config.lora_alpha,
                lora_dropout=self.config.lora_dropout,
                bias="none",
                use_gradient_checkpointing="unsloth",
                random_state=self.config.seed,
            )
            
        else:
            # –ü–æ–ª–Ω–æ–µ —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥
            self.model, self.tokenizer = FastModel.from_pretrained(
                model_name=self.config.model_name,
                max_seq_length=self.config.max_seq_length,
                load_in_4bit=self.config.load_in_4bit,
                load_in_8bit=self.config.load_in_8bit,
                dtype=torch.bfloat16 if self.config.bf16 else torch.float16,
                full_finetuning=True,
            )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º chat template
        self.tokenizer = get_chat_template(
            self.tokenizer,
            chat_template="qwen-3",
        )
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {self.model.get_model_size():,}")
        
    def prepare_training_data(self, train_path: str, balance_classes: bool = False, 
                            balance_method: str = "undersample") -> DatasetDict:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        
        print("üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = pd.read_csv(train_path)
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
        print(df['score'].value_counts().sort_index())
        
        # –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º –∫–ª–∞—Å—Å—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if balance_classes:
            print(f"‚öñÔ∏è –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º –∫–ª–∞—Å—Å—ã –º–µ—Ç–æ–¥–æ–º '{balance_method}'...")
            df = DataPreprocessor(self.tokenizer, self.config).balance_classes(
                df, method=balance_method
            )
            print(f"–ü–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
            print(df['score'].value_counts().sort_index())
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ train/val
        preprocessor = DataPreprocessor(self.tokenizer, self.config)
        train_df, val_df = preprocessor.split_data(df)
        
        print(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(val_df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
        train_dataset = preprocessor.prepare_dataset(train_df)
        val_dataset = preprocessor.prepare_dataset(val_df)
        
        return DatasetDict({
            "train": train_dataset,
            "validation": val_dataset
        })
    
    def setup_trainer(self, dataset: DatasetDict, output_dir: str):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç trainer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        
        training_args = TrainingArguments(
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            per_device_eval_batch_size=self.config.per_device_eval_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            warmup_ratio=self.config.warmup_ratio,
            num_train_epochs=self.config.num_train_epochs,
            learning_rate=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            lr_scheduler_type=self.config.lr_scheduler_type,
            logging_steps=self.config.logging_steps,
            eval_strategy=self.config.eval_strategy,
            eval_steps=self.config.eval_steps,
            save_strategy=self.config.save_strategy,
            save_steps=self.config.save_steps,
            save_total_limit=self.config.save_total_limit,
            fp16=self.config.fp16,
            bf16=self.config.bf16,
            gradient_checkpointing=self.config.gradient_checkpointing,
            dataloader_num_workers=self.config.dataloader_num_workers,
            output_dir=output_dir,
            report_to=["wandb"] if "WANDB_PROJECT" in os.environ else [],
            run_name=self.experiment_name,
        )
        
        self.trainer = SFTTrainer(
            model=self.model,
            tokenizer=self.tokenizer,
            train_dataset=dataset["train"],
            eval_dataset=dataset["validation"],
            dataset_text_field="text",
            max_seq_length=self.config.max_seq_length,
            data_collator=DataCollatorForSeq2Seq(
                tokenizer=self.tokenizer, 
                return_tensors="pt", 
                padding=True
            ),
            args=training_args,
        )
    
    def train(self, train_path: str, output_dir: str, **kwargs):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è"""
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        os.makedirs(output_dir, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º wandb –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if "WANDB_PROJECT" in os.environ:
            wandb.init(
                project=os.environ["WANDB_PROJECT"],
                name=self.experiment_name,
                config=self.config.__dict__
            )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
        self.setup_model_and_tokenizer()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        dataset = self.prepare_training_data(train_path, **kwargs)
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º trainer
        self.setup_trainer(dataset, output_dir)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config_path = os.path.join(output_dir, "training_config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config.__dict__, f, indent=2, ensure_ascii=False)
        
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        self.trainer.train()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...")
        self.trainer.save_model(output_dir)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ unsloth
        self.model.save_pretrained(os.path.join(output_dir, "unsloth_model"))
        self.tokenizer.save_pretrained(os.path.join(output_dir, "unsloth_model"))
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_dir}")
        
        return self.trainer.state.log_history


def run_experiment_1_baseline():
    """–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1: –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å LoRA"""
    
    print("\n" + "="*60)
    print("üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 1: Baseline LoRA Fine-tuning")
    print("="*60)
    
    config = AgentJudgeConfig()
    config.use_lora = True
    config.lora_r = 16
    config.learning_rate = 2e-4
    config.num_train_epochs = 3
    
    trainer = AgentJudgeTrainer(config, "exp1_baseline_lora")
    
    return trainer.train(
        train_path="aij_judge_task_1_train.csv",
        output_dir="models/exp1_baseline_lora",
        balance_classes=False
    )


def run_experiment_2_balanced():
    """–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 2: –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    print("\n" + "="*60)
    print("üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 2: Balanced Classes Training")
    print("="*60)
    
    config = AgentJudgeConfig()
    config.use_lora = True
    config.lora_r = 32  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º LoRA rank
    config.lora_alpha = 64
    config.learning_rate = 1.5e-4
    config.num_train_epochs = 4
    
    trainer = AgentJudgeTrainer(config, "exp2_balanced_lora")
    
    return trainer.train(
        train_path="aij_judge_task_1_train.csv",
        output_dir="models/exp2_balanced_lora", 
        balance_classes=True,
        balance_method="undersample"
    )


def run_experiment_3_full_finetuning():
    """–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 3: –ü–æ–ª–Ω–æ–µ —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥ (–µ—Å–ª–∏ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏)"""
    
    print("\n" + "="*60)
    print("üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 3: Full Fine-tuning")
    print("="*60)
    
    config = AgentJudgeConfig()
    config.use_lora = False  # –ü–æ–ª–Ω–æ–µ —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥
    config.per_device_train_batch_size = 2  # –£–º–µ–Ω—å—à–∞–µ–º batch size
    config.gradient_accumulation_steps = 8  # –ö–æ–º–ø–µ–Ω—Å–∏—Ä—É–µ–º accumulation
    config.learning_rate = 1e-4  # –ú–µ–Ω—å—à–∏–π learning rate
    config.num_train_epochs = 2  # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏
    config.load_in_4bit = True  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    
    trainer = AgentJudgeTrainer(config, "exp3_full_finetuning")
    
    return trainer.train(
        train_path="aij_judge_task_1_train.csv",
        output_dir="models/exp3_full_finetuning",
        balance_classes=True,
        balance_method="oversample"
    )


def run_experiment_4_enhanced():
    """–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 4: –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
    
    print("\n" + "="*60)
    print("üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 4: Enhanced Configuration")
    print("="*60)
    
    config = AgentJudgeConfig()
    config.use_lora = True
    config.lora_r = 64  # –ë–æ–ª—å—à–æ–π rank –¥–ª—è –ª—É—á—à–µ–π –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    config.lora_alpha = 128
    config.lora_dropout = 0.05  # –ú–µ–Ω—å—à–∏–π dropout
    config.learning_rate = 3e-4  # –í—ã—à–µ learning rate
    config.num_train_epochs = 5
    config.warmup_ratio = 0.1
    config.weight_decay = 0.005  # –ú–µ–Ω—å—à–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    config.max_seq_length = 3072  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
    
    trainer = AgentJudgeTrainer(config, "exp4_enhanced_lora")
    
    return trainer.train(
        train_path="aij_judge_task_1_train.csv",
        output_dir="models/exp4_enhanced_lora",
        balance_classes=True,
        balance_method="undersample"
    )


def main():
    parser = argparse.ArgumentParser(description="Agent-as-Judge Model Training")
    parser.add_argument("--experiment", type=str, default="all",
                       choices=["all", "1", "2", "3", "4"],
                       help="–í—ã–±–æ—Ä —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞")
    parser.add_argument("--data_path", type=str, default="aij_judge_task_1_train.csv",
                       help="–ü—É—Ç—å –∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º")
    parser.add_argument("--wandb_project", type=str, default=None,
                       help="–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –≤ W&B –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º W&B –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
    if args.wandb_project:
        os.environ["WANDB_PROJECT"] = args.wandb_project
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
    results = {}
    
    if args.experiment == "all" or args.experiment == "1":
        try:
            results["exp1"] = run_experiment_1_baseline()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ 1: {e}")
    
    if args.experiment == "all" or args.experiment == "2":
        try:
            results["exp2"] = run_experiment_2_balanced()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ 2: {e}")
    
    if args.experiment == "all" or args.experiment == "3":
        try:
            results["exp3"] = run_experiment_3_full_finetuning()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ 3: {e}")
            print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å batch_size –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LoRA")
    
    if args.experiment == "all" or args.experiment == "4":
        try:
            results["exp4"] = run_experiment_4_enhanced()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ 4: {e}")
    
    print("\n" + "="*60)
    print("üéØ –í–°–ï –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´ –ó–ê–í–ï–†–®–ï–ù–´!")
    print("="*60)
    print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ models/")
    print("–î–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π run.py")


if __name__ == "__main__":
    main()
