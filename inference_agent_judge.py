#!/usr/bin/env python3
"""
Agent-as-Judge Model Inference Script
=====================================

–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏-—Å—É–¥—å–∏.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ post-processing.
"""

import os
import pandas as pd
import numpy as np
import torch
from typing import List, Dict, Optional, Union
import argparse
import re
from pathlib import Path
import json
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

# Imports –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—å—é
from transformers import AutoModelForCausalLM, AutoTokenizer
from vllm import LLM, SamplingParams
from unsloth import FastModel
import gc


class AgentJudgeInference:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ Agent-as-Judge –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, model_path: str, device: str = "auto", 
                 use_vllm: bool = True, max_length: int = 10):
        self.model_path = model_path
        self.device = device
        self.use_vllm = use_vllm
        self.max_length = max_length
        self.model = None
        self.tokenizer = None
        
        self.load_model()
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""
        
        print(f"ü§ñ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ {self.model_path}...")
        
        try:
            if self.use_vllm:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º vLLM –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
                self.model = LLM(
                    model=self.model_path,
                    tensor_parallel_size=1,
                    trust_remote_code=True,
                    enforce_eager=True,
                    dtype='bfloat16',
                    max_model_len=2048,
                    gpu_memory_utilization=0.8
                )
                print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å vLLM")
                
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π transformers
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True
                )
                print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å transformers")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å baseline –º–æ–¥–µ–ª—å")
            # Fallback –∫ baseline –º–æ–¥–µ–ª–∏
            self.model_path = "baseline/aij_qwen_0.6b"
            self.load_baseline_model()
    
    def load_baseline_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç baseline –º–æ–¥–µ–ª—å –∫–∞–∫ fallback"""
        
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º baseline –º–æ–¥–µ–ª—å...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        
        if self.use_vllm:
            self.model = LLM(
                model=self.model_path,
                tensor_parallel_size=1,
                trust_remote_code=True,
                enforce_eager=True,
                dtype='bfloat16'
            )
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.bfloat16,
                device_map=self.device,
                trust_remote_code=True
            )
    
    def format_prompt_for_inference(self, prompt: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        system_prompt = """–¢—ã - –º–æ–¥–µ–ª—å-—Å—É–¥—å—è, –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –≤—ã—Å—Ç–∞–≤—å –æ—Ü–µ–Ω–∫—É —Å–æ–≥–ª–∞—Å–Ω–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é –∏ —à–∫–∞–ª–µ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è. –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–º –æ—Ç 0 –¥–æ 3 (–∏–ª–∏ -1, –µ—Å–ª–∏ –∫—Ä–∏—Ç–µ—Ä–∏–π –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º)."""
        
        conversation = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]
        
        formatted = self.tokenizer.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False
        )
        
        return formatted
    
    def predict_batch_vllm(self, prompts: List[str]) -> List[str]:
        """Batch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å vLLM"""
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç—ã
        formatted_prompts = [self.format_prompt_for_inference(p) for p in prompts]
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        sampling_params = SamplingParams(
            max_tokens=self.max_length,
            temperature=0.0,  # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
            top_p=1.0,
            frequency_penalty=0.0,
            presence_penalty=0.0,
            stop_token_ids=[self.tokenizer.eos_token_id]
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã
        outputs = self.model.generate(formatted_prompts, sampling_params)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã
        results = [output.outputs[0].text.strip() for output in outputs]
        
        return results
    
    def predict_batch_transformers(self, prompts: List[str]) -> List[str]:
        """Batch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å transformers"""
        
        results = []
        
        for prompt in tqdm(prompts, desc="–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã"):
            formatted_prompt = self.format_prompt_for_inference(prompt)
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(
                formatted_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=2048
            ).to(self.model.device)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.max_length,
                    temperature=0.0,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            results.append(generated_text)
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            del inputs, outputs
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        return results
    
    def extract_score_from_text(self, text: str) -> int:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ–≤—É—é –æ—Ü–µ–Ω–∫—É –∏–∑ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        text = text.strip()
        
        # –ò—â–µ–º —Ü–∏—Ñ—Ä—ã –≤ –Ω–∞—á–∞–ª–µ —Ç–µ–∫—Å—Ç–∞
        if text and text[0].isdigit():
            score = int(text[0])
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏
            if score in [0, 1, 2, 3] or score == -1:
                return score
        
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å –æ—Ü–µ–Ω–∫–∞–º–∏
        patterns = [
            r'^\s*(-1|[0-3])\s*$',  # –¢–æ–ª—å–∫–æ —á–∏—Å–ª–æ
            r'–û—Ü–µ–Ω–∫–∞:\s*(-1|[0-3])',  # "–û—Ü–µ–Ω–∫–∞: X"
            r'–ë–∞–ª–ª:\s*(-1|[0-3])',    # "–ë–∞–ª–ª: X"
            r'Score:\s*(-1|[0-3])',   # "Score: X"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 0 (–Ω–∞–∏–±–æ–ª–µ–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
        return 0
    
    def predict(self, prompts: List[str], batch_size: int = 32) -> List[int]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ü–µ–Ω–æ–∫"""
        
        all_scores = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
        for i in tqdm(range(0, len(prompts), batch_size), desc="–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∏"):
            batch_prompts = prompts[i:i + batch_size]
            
            if self.use_vllm:
                batch_results = self.predict_batch_vllm(batch_prompts)
            else:
                batch_results = self.predict_batch_transformers(batch_prompts)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ü–µ–Ω–∫–∏
            batch_scores = [self.extract_score_from_text(result) for result in batch_results]
            all_scores.extend(batch_scores)
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        return all_scores
    
    def evaluate_on_validation(self, val_data_path: str) -> Dict:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        print("üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = pd.read_csv(val_data_path)
        prompts = df['prompt'].tolist()
        true_scores = df['score'].tolist()
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predicted_scores = self.predict(prompts)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
        
        accuracy = accuracy_score(true_scores, predicted_scores)
        f1 = f1_score(true_scores, predicted_scores, average='weighted')
        
        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
        report = {
            'accuracy': accuracy,
            'f1_score': f1,
            'classification_report': classification_report(
                true_scores, predicted_scores, output_dict=True
            ),
            'confusion_matrix': confusion_matrix(true_scores, predicted_scores).tolist()
        }
        
        print(f"Accuracy: {accuracy:.4f}")
        print(f"F1-score: {f1:.4f}")
        
        return report


def main():
    parser = argparse.ArgumentParser(description="Agent-as-Judge Model Inference")
    parser.add_argument("--test_path", type=str, required=True,
                       help="–ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É CSV —Ñ–∞–π–ª—É")
    parser.add_argument("--pred_path", type=str, required=True,
                       help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    parser.add_argument("--model_path", type=str, 
                       default="models/exp1_baseline_lora/unsloth_model",
                       help="–ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
    parser.add_argument("--batch_size", type=int, default=32,
                       help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞")
    parser.add_argument("--use_vllm", action="store_true", default=True,
                       help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å vLLM –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞")
    parser.add_argument("--max_length", type=int, default=10,
                       help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
    parser.add_argument("--validate", type=str, default=None,
                       help="–ü—É—Ç—å –∫ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    if not os.path.exists(args.model_path):
        print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {args.model_path}")
        print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º baseline –º–æ–¥–µ–ª—å...")
        args.model_path = "baseline/aij_qwen_0.6b"
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º inference
        predictor = AgentJudgeInference(
            model_path=args.model_path,
            use_vllm=args.use_vllm,
            max_length=args.max_length
        )
        
        # –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—è
        if args.validate:
            report = predictor.evaluate_on_validation(args.validate)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
            report_path = args.pred_path.replace('.csv', '_validation_report.json')
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"üìã –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {args.test_path}")
        test_df = pd.read_csv(args.test_path)
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(test_df)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        print("üîÆ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
        predictions = predictor.predict(
            test_df['prompt'].tolist(),
            batch_size=args.batch_size
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results_df = pd.DataFrame({
            'id': test_df['id'],
            'score': predictions
        })
        
        results_df.to_csv(args.pred_path, index=False)
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {args.pred_path}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        print("\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
        print(results_df['score'].value_counts().sort_index())
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
